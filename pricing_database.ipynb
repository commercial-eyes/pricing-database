{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "from sys import exit\n",
    "\n",
    "import requests\n",
    "from requests_html import HTML\n",
    "\n",
    "from pathlib import Path\n",
    "from urlpath import URL\n",
    "import string\n",
    "\n",
    "\n",
    "BASE_DIR = Path(r'C:\\Python\\AEMP')\n",
    "BASE_URL = URL('https://www.pbs.gov.au')\n",
    "exman_prices_url = BASE_URL / 'info/industry/pricing/ex-manufacturer-price'\n",
    "  \n",
    "\n",
    "def timeis(func):\n",
    "    '''Decorator that reports the execution time.'''\n",
    "  \n",
    "    def wrap(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end = time.time()\n",
    "          \n",
    "        print(func.__name__, end-start)\n",
    "        return result\n",
    "    \n",
    "    return wrap\n",
    "\n",
    "\n",
    "def read_download_log(BASE_DIR):\n",
    "    \"\"\"\n",
    "    Returns the logged HREFs that have already been imported into the\n",
    "    pricing database.\n",
    "    \"\"\"\n",
    "    \n",
    "    path = Path(BASE_DIR) / 'logs' / 'download_log.json'\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if path.exists():\n",
    "        with open(path, 'r') as log:\n",
    "            return json.load(log)\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "    \n",
    "def write_download_log(BASE_DIR, log_data):\n",
    "    \"\"\"\n",
    "    Write the HREFs that have been imported into the pricing database\n",
    "    to the download log.\n",
    "    \"\"\"\n",
    "    \n",
    "    path = Path(BASE_DIR) / 'logs' / 'download_log.json'\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, 'w') as log:\n",
    "        json.dump(log_data, log)\n",
    "    \n",
    "    \n",
    "def check_for_updates(BASE_DIR, exman_prices_url):\n",
    "    \"\"\"\n",
    "    Checks the current exman-prices download page for new data that isn't stored in the\n",
    "    download log. Returns a list of hrefs that need to be imported.\n",
    "    \"\"\"\n",
    "    \n",
    "    download_log = read_download_log(BASE_DIR)\n",
    "    r = requests.get(exman_prices_url)\n",
    "    html = HTML(html=r.content)\n",
    "\n",
    "    dataset_urls = [ele.attrs['href'] for ele in html.find('a') \n",
    "                    if (ele.attrs['href'].lower().__contains__('xls')) and\n",
    "                    (ele.attrs['href'] not in download_log)]\n",
    "    \n",
    "    return dataset_urls\n",
    "\n",
    "\n",
    "def read_exman_source(url):\n",
    "    \"\"\"\n",
    "    Reads source exman .xls file, including date and schedule from url,\n",
    "    and returns a cleaned df \n",
    "    \"\"\"\n",
    "\n",
    "    def read_exman_date(url):\n",
    "        \"\"\"\n",
    "        Returns the date of the dataset passed in the url in datetime format\n",
    "        \"\"\"\n",
    "\n",
    "        url = URL(url)\n",
    "\n",
    "        return pd.to_datetime('-'.join(url.stem.split('-')[-3:]))\n",
    "\n",
    "\n",
    "    def read_exman_schedule(url):\n",
    "        \"\"\"\n",
    "        Returns the schedule of the dataset passed in the url (efc or non-efc)\n",
    "        \"\"\"\n",
    "\n",
    "        url = URL(url)\n",
    "\n",
    "        if url.stem.partition('prices-')[2].partition('-')[0] == 'efc':\n",
    "            return 'efc'\n",
    "        else:\n",
    "            return 'non-efc'\n",
    "\n",
    "    date = read_exman_date(url)\n",
    "    schedule = read_exman_schedule(url)\n",
    "    df = pd.read_excel(url)\n",
    "    df['Date'] = date\n",
    "    df['Schedule'] = schedule\n",
    "\n",
    "    return clean_df(df, schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AMTNameError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def clean_df(df, schedule):\n",
    "    \"\"\"\n",
    "    Cleans df, with different actions based on schedule  \n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    def catch_amt_name(col):\n",
    "        \"\"\"\n",
    "        Used to catch the AMT trade product pack (TPP) name for standardisation,\n",
    "        as it changes across time\n",
    "        \"\"\"\n",
    "        \n",
    "        catch = ('amt', 'tpp', 'product pack')\n",
    "        return any(x in col.lower() for x in catch)\n",
    "    \n",
    "    \n",
    "    df.columns = [col.strip() for col in df.columns] # clean whitespace\n",
    "    \n",
    "    AMT_name = list(filter(catch_amt_name, df.columns))\n",
    "    if not AMT_name:\n",
    "        raise AMTNameError(f'AMT name not found')\n",
    "    if len(AMT_name) > 1:\n",
    "        raise AMTNameError('len(AMT_name) > 1')\n",
    "    \n",
    "    AMT_name = AMT_name[0]\n",
    "    df.rename(columns={AMT_name: 'AMT TPP'}, inplace=True)\n",
    "        \n",
    "    if 'C\\'wlth Pays Premium' in df.columns:\n",
    "        df.rename(columns={'C\\'wlth Pays Premium': 'Commonwealth Pays Premium'}, inplace=True)\n",
    "\n",
    "    rename_cols = {\n",
    "        'Maximum Amount': 'Maximum Quantity/Amount',\n",
    "        'Maximum Quantity': 'Maximum Quantity/Amount',\n",
    "        'Number Repeats': 'Maximum/Number Repeats',\n",
    "        'Maximum Repeats': 'Maximum/Number Repeats',\n",
    "        'DPMA': 'DPMQ/DPMA',\n",
    "        'DPMQ':'DPMQ/DPMA',\n",
    "        'Claimed DPMA': 'Claimed DPMQ/DPMA',\n",
    "        'Claimed DPMQ':'Claimed DPMQ/DPMA'\n",
    "    }\n",
    "    \n",
    "    rename_cols = {k: v for k, v in rename_cols.items() if k in df.columns}\n",
    "    df.rename(columns=rename_cols, inplace=True)\n",
    "\n",
    "    # depreceated/misnamed columns to drop\n",
    "    columns_to_drop = [\n",
    "        'index', 'AMT Trade Product Pack Pack', 'Exempt', 'Therapeutic Group',\n",
    "        'New PI or Brand', 'Previous Pricing Quantity', 'Previous AEMP', 'Price Change Event',\n",
    "        'Previous Premium', 'ATC', 'DD', 'MRVSN', 'Substitutable', ' Item Code',\n",
    "        'Authorised Rep', 'Email', 'AMT Trade Product pack', 'AMT Trade product Pack',\n",
    "        'ANT Trade Product Pack', 'TPP', 'AMT Trade Product Pack ', 'Amt Trade Product Pack'\n",
    "    ]\n",
    "        \n",
    "    columns_to_drop = [field for field in filter(columns_to_drop.__contains__, df.columns)]\n",
    "    df.drop(columns_to_drop, axis=1, inplace=True)\n",
    "        \n",
    "    # set commonwealth pays premium to a boolean\n",
    "    if 'Commonwealth Pays Premium' in df.columns:\n",
    "        df['Commonwealth Pays Premium'] = df['Commonwealth Pays Premium'].apply(\n",
    "            lambda x: True if x == 'Yes' else False\n",
    "        )\n",
    "    \n",
    "    # clean up premium type and premium value\n",
    "    float_chars = '1234567890.'\n",
    "    if 'Premium' in df.columns:\n",
    "        df['Premium Type'] = df['Premium'].apply(lambda x: ''.join(filter(string.ascii_letters.__contains__, str(x))))\n",
    "        df['Premium'] = df['Premium'].apply(lambda x: ''.join(filter(float_chars.__contains__,str(x))))\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_updates(BASE_DIR, exman_prices_url):\n",
    "    \"\"\"\n",
    "    Downloads any missing datasets from the PBS and returns these as a df\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_urls = check_for_updates(BASE_DIR, exman_prices_url)\n",
    "    if not dataset_urls:\n",
    "        return None\n",
    "\n",
    "    new_data = []\n",
    "    for url in tqdm(dataset_urls, desc='Importing new data'):\n",
    "        new_data.append(read_exman_source(BASE_URL / url))\n",
    "\n",
    "    df = pd.concat(new_data, sort=False)\n",
    "    \n",
    "    # log updates\n",
    "    log = read_download_log(BASE_DIR)\n",
    "    log.extend(dataset_urls)\n",
    "    write_download_log(BASE_DIR, log)\n",
    "    \n",
    "    return df.reset_index(drop=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_db(BASE_DIR, latest_month_only=False, name='db'):\n",
    "    \"\"\"\n",
    "    Loads local pricing database\n",
    "    \"\"\"\n",
    "    \n",
    "    path = BASE_DIR / name\n",
    "    if not path.exists():\n",
    "        return None\n",
    "    \n",
    "    df = pd.read_feather(path)\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    \n",
    "    if latest_month_only:\n",
    "        df = df.loc[df.Date == df.Date.max()]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def write_db(BASE_DIR, df, append=False, name='db'):\n",
    "    \"\"\"\n",
    "    Writes dataframe to local pricing database, or appends to existing\n",
    "    \"\"\"\n",
    "    \n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    if not append:\n",
    "        df.to_feather(BASE_DIR / 'db')\n",
    "    else:\n",
    "        df_base = load_db(BASE_DIR)\n",
    "        df = pd.concat([df_base, df], sort=False)\n",
    "        df.reset_index(drop=True).to_feather(BASE_DIR / 'db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeis\n",
    "def perform_lookup(df, item_map, atc_map):\n",
    "    \"\"\"\n",
    "    Generate Unique SKU IDs for generating previous AEMPs and creating longitudinal relationships\n",
    "    \"\"\"\n",
    "\n",
    "    lookup_cols = [\n",
    "        'Item Code', 'AMT TPP', 'Pack Quantity', 'Pricing Quantity', \n",
    "        'Vial Content', 'Maximum Quantity/Amount', 'Number/Maximum Repeats'\n",
    "    ]\n",
    "\n",
    "    # filter to cols that are present in df\n",
    "    print('Creating lookup column...')\n",
    "    lookup_cols = [col for col in lookup_cols if col in df.columns]\n",
    "    df['SKU ID'] = df[lookup_cols].applymap(str).agg('-'.join,axis=1)\n",
    "\n",
    "    # generate previous AEMPs\n",
    "    print('Calculating previous prices...')\n",
    "    df['Previous AEMP'] = df.groupby('SKU ID')['AEMP'].shift(fill_value=np.nan)\n",
    "\n",
    "    # indicators of increase/decrease for analysis\n",
    "    conditions_increase = [\n",
    "      (df['AEMP'] > df['Previous AEMP']) & (df['Previous AEMP'] != np.nan),\n",
    "      (df['AEMP'] <= df['Previous AEMP']) & (df['Previous AEMP'] != np.nan)\n",
    "    ]\n",
    "    conditions_decrease = [\n",
    "      (df['AEMP'] < df['Previous AEMP']) & (df['Previous AEMP'] != np.nan),\n",
    "      (df['AEMP'] >= df['Previous AEMP']) & (df['Previous AEMP'] != np.nan)\n",
    "    ]\n",
    "    outcomes = [True, False]\n",
    "\n",
    "    # numpy - return 1,0 for increase/decrease conditionals\n",
    "    df['AEMP_Increase'] = np.select(conditions_increase, outcomes, default=False)\n",
    "    df['AEMP_Decrease'] = np.select(conditions_decrease, outcomes, default=False)\n",
    "\n",
    "    # absolute and relative change calculations\n",
    "    df['AEMP_Abs_Change'] = df['AEMP'] - df['Previous AEMP']\n",
    "    df['AEMP_Rel_Change'] = (df['AEMP'] - df['Previous AEMP']) / df['Previous AEMP']\n",
    "\n",
    "    # generate 6 dig item code for map lookup\n",
    "    print('Generating ATC labels...')\n",
    "    \n",
    "    df['ItemCodeLookup'] = df['Item Code'].map(lambda x: '0'*(6-len(str(x)))+str(x))\n",
    "\n",
    "    df = df.merge(item_map[['ITEM_CODE','ATC_Code']], how='left', left_on='ItemCodeLookup', right_on='ITEM_CODE').drop('ITEM_CODE',axis=1)\n",
    "    df.rename(columns={'ATC5_Code':'ATC_Code'},inplace=True)\n",
    "\n",
    "    # create levelled ATC codes\n",
    "    df['ATC1'] = df['ATC_Code'].str[0]\n",
    "    df['ATC3'] = df['ATC_Code'].str[:3]\n",
    "    df['ATC4'] = df['ATC_Code'].str[:4]\n",
    "    df['ATC5'] = df['ATC_Code'].str[:5]\n",
    "\n",
    "    for ATC_level in tqdm(['ATC1','ATC3','ATC4','ATC5','ATC_Code']):\n",
    "        df = df.merge(atc_map[['ATC Code', 'Label']],\n",
    "                      how='left',\n",
    "                      left_on=ATC_level,\n",
    "                      right_on='ATC Code').drop('ATC Code', axis=1)\n",
    "        df.rename(columns={'Label': ATC_level + '_label'}, inplace=True)\n",
    "\n",
    "    df.drop('ItemCodeLookup',axis=1,inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PBSData:\n",
    "    \"\"\"\n",
    "    Wrapper for methods to grab various data from the PBS website\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.BASE_DIR = Path(r'C:\\Python\\AEMP')\n",
    "        self.BASE_URL = URL('https://www.pbs.gov.au')\n",
    "        self.source_url = BASE_URL / 'info/browse/download'\n",
    "        self.item_drug_map_url = BASE_URL / '/statistics/dos-and-dop/files/pbs-item-drug-map.csv'\n",
    "        self.exman_prices_url = BASE_URL / 'info/industry/pricing/ex-manufacturer-price'\n",
    "\n",
    "        \n",
    "    def get_latest_PBS_text_files(self):\n",
    "        \"\"\"\n",
    "        Returns a ZipFile of the most recent PBS text files\n",
    "        \"\"\"\n",
    "\n",
    "        r = requests.get(self.source_url)\n",
    "        html = HTML(html=r.content)\n",
    "\n",
    "        # filter to current PBS text files .zip\n",
    "        href = [ele for ele in html.find('a.xref') \n",
    "                         if ('PBS Text files' in ele.attrs['title'])\n",
    "                         and ('.zip' in ele.attrs['href'].lower())][0].attrs['href']\n",
    "        discard, sep, url = href.partition('downloads')\n",
    "        zip_url = sep + url\n",
    "\n",
    "        r = requests.get(BASE_URL / zip_url, stream=True)\n",
    "        return ZipFile(BytesIO(r.content))\n",
    "\n",
    "\n",
    "    def get_atc_from_text_files(self, zipfile):\n",
    "        \"\"\"\n",
    "        Gets ATC code map from PBS text files and returns as df with cols ('ATC', 'Description')\n",
    "        \"\"\"\n",
    "\n",
    "        zipfile_dir = zipfile.namelist()\n",
    "        for file in zipfile_dir:\n",
    "            if 'atc_' in file:\n",
    "                break\n",
    "\n",
    "        with zipfile.open(file) as f:\n",
    "            data = [line.decode('utf-8').strip().split('!') for line in f.readlines()][1:]\n",
    "\n",
    "        return pd.DataFrame(data, columns=['ATC Code', 'Label'])\n",
    "\n",
    "\n",
    "    def get_item_drug_map(self):\n",
    "        \"\"\"\n",
    "        Returns a df of the PBS item drug map\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        def strip_leading_zeros(x):\n",
    "            \"\"\"\n",
    "            Strips leading zeros from item code\n",
    "            \"\"\"\n",
    "\n",
    "            while x[0] == '0':\n",
    "                x = x[1:]\n",
    "            return x\n",
    "\n",
    "        df = pd.read_csv(self.item_drug_map_url, encoding='latin-1')\n",
    "        df.columns = ['ITEM_CODE', 'DRUG_NAME', 'PRESENTATION', 'ATC_Code']\n",
    "\n",
    "        return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new data found, exiting.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ian.teichert\\Desktop\\Local Python\\WinPython\\WPy64-3760\\python-3.7.6.amd64\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3327: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "new_data = download_updates(BASE_DIR, exman_prices_url)\n",
    "if not isinstance(new_data, pd.DataFrame):\n",
    "    print('No new data found, exiting.')\n",
    "    exit()\n",
    "    \n",
    "earliest_date = new_data.Date.min()\n",
    "old_data = load_db(BASE_DIR, latest_month_only=True)\n",
    "\n",
    "APPEND = True\n",
    "\n",
    "if not isinstance(old_data, pd.DataFrame) or not old_data.Date.max() < earliest_date:\n",
    "    df = new_data\n",
    "    APPEND = False\n",
    "    del old_data\n",
    "else:\n",
    "    df = pd.concat([old_data, new_data], join='inner', sort=False)\n",
    "    \n",
    "pbs_data = PBSData()\n",
    "text_files_zip = pbs_data.get_latest_PBS_text_files()\n",
    "atc_map = pbs_data.get_atc_from_text_files(text_files_zip)\n",
    "item_map = pbs_data.get_item_drug_map()\n",
    "\n",
    "df = perform_lookup(df, item_map, atc_map)\n",
    "\n",
    "if APPEND:\n",
    "    df = df.loc[df.Date != df.Date.min()]\n",
    "    \n",
    "write_db(BASE_DIR, df, append=APPEND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
